<!DOCTYPE html>
<html lang="en" class="h-100">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="description" content="" />
    <meta name="author" content="Ayman Kassab, Frank Shi, and Sahil Pahooja" />
    <meta name="generator" content="Hugo 0.84.0" />
    <title>AI · Breaker - Experiments</title>

    <!-- Bootstrap core CSS -->
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC"
      crossorigin="anonymous"
    />

    <style>
      .bd-placeholder-img {
        font-size: 1.125rem;
        text-anchor: middle;
        -webkit-user-select: none;
        -moz-user-select: none;
        user-select: none;
      }

      @media (min-width: 768px) {
        .bd-placeholder-img-lg {
          font-size: 3.5rem;
        }
      }
    </style>

    <!-- Custom styles for this template -->
    <!-- <link href="cover.css" rel="stylesheet" /> -->
    <link href="{{ url_for('static',filename='styles/cover.css') }}" rel="stylesheet" />
  </head>
  <body class="d-flex h-100 text-center text-white bg-dark">
    <div class="cover-container d-flex w-100 h-100 p-3 mx-auto flex-column">
      <header class="mb-auto">
        <div>
          <h3 class="float-md-start mb-0">AI · Breaker</h3>
          <nav class="nav nav-masthead justify-content-center float-md-end">
            <a class="nav-link" href={{url_for("home")}}>Experiments</a>
            <a class="nav-link active" aria-current="page" href="#">Social Impact</a>
            <a class="nav-link" href={{url_for("tryme")}}>Try Your Own!</a>
          </nav>
        </div>
      </header>

      <main class="px-3">
        <h1 class="mt-5">Social Impact</h1>
        <div class="row align-items-md-stretch mt-5">
          <div class="col-md-12 mb-5">
            <div class="h-100 p-5 text-white bg-dark border border-white rounded-3">
              <h2 class="text-start">Social Implications:</h2>
              <p class="text-start">
                The adversarial attack is a commonly known issue in the AI industry nowadays. 
                Even if sometimes we don't have access to the inside of a model, 
                a hacker can "guess" the model's architecture. It is a fact that many machine 
                learning models currently all have similar architectures and training procedures,
                 and most of them are open to the public. Most training datasets are also shared,
                  for example, ImageNet. In addition, transfer learning is popularly used, 
                  using a pre-trained model as a basis or starting point of a new model. 
                  Some research has shown that if an adversarial example impacts shallow models, 
                  it could also impact a model with similar architectures.

                  <h5 class="text-start">Safety</h5>
                  <p class="text-start">
                  Safety is the most direct impact of this issue. A group of researchers (Athalye et. al (2017))
                   has shown they can 3d-print a well-designed toy turtle to make an object 
                   detection camera think it is a rifle. Similarly, a rifle can be modified 
                   and make an object detection algorithm think it is a turtle or other instances. 
                   <a href="https://www.youtube.com/watch?v=piYnd_wYlT8"> Fooling Image Recognition with Adversarial Examples</a>
                  </p>
                   <h5 class="text-start">Reliability</h5>
                   <p class="text-start">
                    If an AI model can be tricked by tiny modifications on the input, 
                    we should ask, how “reliable is an AI and how much we can trust them”? 
                    It is reasonable to think further: if an AI model makes such mistakes, 
                    who is responsible for the cost? Can AI really replace humans?
                   </p>
                   <h5 class="text-start">Security</h5>
                   <p class="text-start">
                     Consider authorization AI like a card-id scanner 
                     or a face detection system deployed in private enterprise. A
                      card-id scanner could be replaced by a counterfeit card-id 
                      with the same photo with some added noise tricking. This could bring us 
                      security concerns if human is replaced by AI in security fields.
                   </p>




          
              </p>
            </div>
          </div>
          <div class="col-md-12 mb-5">
            <div class="h-100 p-5 text-white bg-dark border border-white rounded-3">
              <h2 class="text-start">Defense:</h2>
              <p class="text-start">
                Developers should consider more about adversarial attacks like we have 
                introduced and other concerns, including privacy leakage, when training 
                AI models instead of focusing only on accuracy. And this is the main goal
                 of our project, to make AI developers aware of the vulnerabilities in AI models.
                 <h5 class="text-start">Defense when training:</h5>
                   <p class="text-start"> This is known as Adversarial Training,
                      where the developers take the impact of adversarial examples
                       into account when building and training the models. Also, it 
                       is encouraged to use modified architectures when building AI models
                        instead of copying from public research papers directly, which could 
                        add some difficulties to attackers.
                   </p>
                   <h5 class="text-start">Defense when benchmarking and deploying:</h5>
                   <p class="text-start"> One (a company or developer) should design a professional
                     benchmarking process on the robustness (and other issues, like information leakage) before releasing a model,
                     in order to catch any issues in advance. 
                   </p>
                   <br>
                   <p class="text-start">
                   In general, it is challenging to build a perfectly robust 
                   AI model as defence is much difficult than attacking 
                   (A good defence approach needs to protect against all kinds 
                   of attacks, but a good attack only needs to break one defence 
                   approach). Nevertheless, it is glad to see more researchers are 
                   starting to study the robustness of AI models these days.
                  </p>

              </p>
            </div>
          </div>
        </div>
      </main>

      <footer class="mt-auto text-white-50">
        <p class="text-white">For University of Waterloo - CS 492 - Spring 2021</p>
      </footer>
    </div>

    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
      integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
      crossorigin="anonymous"
    ></script>
  </body>
</html>
